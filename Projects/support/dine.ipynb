{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cab12a8-682a-42c1-9f84-72e6d393d031",
   "metadata": {},
   "source": [
    "Given the conceptual nature of the Directed Information (DI) Neural Estimator (DINE) and the absence of a widely recognized standard implementation for such a specific task, I'll guide you through a hypothetical example of how one might approach creating a DINE using Julia with the Flux.jl library. This example will focus on a simplified scenario where we aim to estimate the directed information between two sequences using a neural network. The goal is to highlight key components rather than provide a fully operational or optimized solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca2d0d-fdbe-47e4-8918-b09e018c49cf",
   "metadata": {},
   "source": [
    "\n",
    "### Preliminary Setup\n",
    "\n",
    "First, ensure you have Julia installed on your computer along with the Flux.jl package for neural networks and Zygote.jl for automatic differentiation, which is a dependency of Flux but can be used directly if needed for custom gradients.\n",
    "\n",
    "```julia\n",
    "using Pkg\n",
    "Pkg.add(\"Flux\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcda395-8412-47d6-b472-ee710408df07",
   "metadata": {},
   "source": [
    "### Step 1: Define the Neural Network Model\n",
    "\n",
    "We'll define a simple RNN model to process sequences. This model aims to capture temporal dependencies and interactions between two sequences, X and Y, to estimate the directed information from X to Y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "590d177a-52d7-4d4b-a6f2-77c655934e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e419c8-8bad-4ce0-979f-2ac0a784d9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dense(40 => 1)      \u001b[90m# 41 parameters\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a model with two RNN layers for each sequence and a Dense layer to output a single value\n",
    "model_x = RNN(10, 20)  # RNN for processing sequence X, where 10 is input size, and 20 is the hidden layer size\n",
    "model_y = RNN(10, 20)  # RNN for processing sequence Y with the same dimensions\n",
    "dense_output = Dense(40, 1)  # Combines the outputs of both RNNs and produces a single value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ccc696f-9b08-4671-94a3-64a5b9475f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function model(x, y)\n",
    "    x_processed = model_x(x)\n",
    "    y_processed = model_y(y)\n",
    "    combined = vcat(x_processed, y_processed)  # Concatenate the outputs\n",
    "    return sum(dense_output(combined))  # Ensure scalar output by summing, if multiple values are expected\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0fd9c-e38d-417a-a209-bf34ccb61ba2",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Define a Loss Function\n",
    "\n",
    "In a real DINE implementation, the loss function would need to specifically address the estimation of directed information. For simplicity, let's define a placeholder loss function that encourages the model to differentiate between sequences with high and low directed information. Note: In practice, this would involve a more sophisticated approach, possibly involving mutual information estimations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6b6c85-7529-4f32-bc4c-1c80c223d07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_function (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss_function(x, y, true_di)\n",
    "    estimated_di = model(x, y)\n",
    "    # Assuming estimated_di returns a vector with a single element, use broadcasting for subtraction\n",
    "    # Mean squared error loss\n",
    "    return (estimated_di[1] - true_di)^2  # Corrected line\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a29d3-fe56-4a5b-85f7-fd60d369f2a8",
   "metadata": {},
   "source": [
    "### Step 3: Data Preparation\n",
    "\n",
    "For demonstration, we'll assume `data_x` and `data_y` are sequences with some form of directed information from X to Y, and `true_di_values` represents the true directed information (highly simplified and hypothetical).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba4ca0b-e4ae-4326-8cea-b614f2b7deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = [rand(Float32, 10) for _ in 1:100]  # 100 sequences of 10 elements each\n",
    "data_y = [rand(Float32, 10) for _ in 1:100]\n",
    "true_di_values = rand(Float32, 100);  # Placeholder for true DI values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0995f-0cb3-41d2-98b7-1e8f174404de",
   "metadata": {},
   "source": [
    "### Step 4: Training Loop\n",
    "\n",
    "Hereâ€™s a simplified training loop:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90a4f3bb-2af0-4612-abb1-84b68d39c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Flux: params, train!\n",
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "907f8c71-1dda-4b20-a316-ee3193be0c7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching batch_loss(::Vector{Float32}, ::Vector{Float32}, ::Float32)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  batch_loss(::Any)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[32mMain\u001b[39m \u001b[90m\u001b[4mIn[14]:31\u001b[24m\u001b[39m\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching batch_loss(::Vector{Float32}, ::Vector{Float32}, ::Float32)\n\n\u001b[0mClosest candidates are:\n\u001b[0m  batch_loss(::Any)\n\u001b[0m\u001b[90m   @\u001b[39m \u001b[32mMain\u001b[39m \u001b[90m\u001b[4mIn[14]:31\u001b[24m\u001b[39m\n",
      "",
      "Stacktrace:",
      "  [1] macro expansion",
      "    @ ~/.julia/packages/Zygote/jxHJc/src/compiler/interface2.jl:0 [inlined]",
      "  [2] _pullback(::Zygote.Context{true}, ::typeof(batch_loss), ::Vector{Float32}, ::Vector{Float32}, ::Float32)",
      "    @ Zygote ~/.julia/packages/Zygote/jxHJc/src/compiler/interface2.jl:81",
      "  [3] _apply(::Function, ::Vararg{Any})",
      "    @ Core ./boot.jl:838",
      "  [4] adjoint",
      "    @ ~/.julia/packages/Zygote/jxHJc/src/lib/lib.jl:203 [inlined]",
      "  [5] _pullback",
      "    @ ~/.julia/packages/ZygoteRules/M4xmc/src/adjoint.jl:67 [inlined]",
      "  [6] #37",
      "    @ ~/.julia/packages/Flux/ljuc2/src/optimise/train.jl:92 [inlined]",
      "  [7] _pullback(::Zygote.Context{true}, ::Flux.Optimise.var\"#37#40\"{typeof(batch_loss), Tuple{Vector{Float32}, Vector{Float32}, Float32}})",
      "    @ Zygote ~/.julia/packages/Zygote/jxHJc/src/compiler/interface2.jl:0",
      "  [8] pullback(f::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})",
      "    @ Zygote ~/.julia/packages/Zygote/jxHJc/src/compiler/interface.jl:465",
      "  [9] withgradient(f::Function, args::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})",
      "    @ Zygote ~/.julia/packages/Zygote/jxHJc/src/compiler/interface.jl:205",
      " [10] macro expansion",
      "    @ ~/.julia/packages/Flux/ljuc2/src/optimise/train.jl:91 [inlined]",
      " [11] macro expansion",
      "    @ ~/.julia/packages/ProgressLogging/6KXlp/src/ProgressLogging.jl:328 [inlined]",
      " [12] train!(loss::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, data::Vector{Tuple{Vector{Float32}, Vector{Float32}, Float32}}, opt::Adam; cb::Flux.Optimise.var\"#38#41\")",
      "    @ Flux.Optimise ~/.julia/packages/Flux/ljuc2/src/optimise/train.jl:90",
      " [13] train!(loss::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, data::Vector{Tuple{Vector{Float32}, Vector{Float32}, Float32}}, opt::Adam)",
      "    @ Flux.Optimise ~/.julia/packages/Flux/ljuc2/src/optimise/train.jl:86",
      " [14] top-level scope",
      "    @ ./In[14]:43"
     ]
    }
   ],
   "source": [
    "# optimizer = ADAM(0.01)  # Using the ADAM optimizer\n",
    "# data = zip(data_x, data_y, true_di_values)\n",
    "\n",
    "# for epoch in 1:10\n",
    "#     for (x, y, true_di) in data\n",
    "#         loss = loss_function(x, y, true_di)\n",
    "#         gradients = gradient(params(model_x, model_y, dense_output)) do\n",
    "#             loss_function(x, y, true_di)\n",
    "#         end\n",
    "#         train!(optimizer, params(model_x, model_y, dense_output), gradients)\n",
    "#     end\n",
    "#     println(\"Epoch $epoch, Loss: $(loss)\")\n",
    "# end\n",
    "\n",
    "# using Flux\n",
    "\n",
    "# Assuming model_x, model_y, dense_output, and data are already defined\n",
    "\n",
    "optimizer = ADAM(0.01)  # Using the ADAM optimizer\n",
    "\n",
    "# Define a loss function that accepts inputs and a true DI value\n",
    "function loss(x, y, true_di)\n",
    "    estimated_di = model(x, y) # Your model function here\n",
    "    return (estimated_di[1] - true_di)^2  # Ensure scalar output and compute MSE\n",
    "end\n",
    "\n",
    "# Wrap parameters of all models into one collection\n",
    "params = Flux.params(model_x, model_y, dense_output)\n",
    "\n",
    "# Define a function that Flux.train! can use to compute the loss for a given batch of data\n",
    "function batch_loss(data)\n",
    "    # Unpack the data\n",
    "    x, y, true_di = data\n",
    "    return loss(x, y, true_di)\n",
    "end\n",
    "\n",
    "# Assuming data_x, data_y, and true_di_values are defined somewhere above\n",
    "data = [(data_x[i], data_y[i], true_di_values[i]) for i in 1:length(data_x)]\n",
    "\n",
    "# Training loop\n",
    "for epoch in 1:10\n",
    "    # Use Flux.train! with a custom batch loss function and your data\n",
    "    Flux.train!(batch_loss, params, data, optimizer)\n",
    "    # Optionally print out the loss here - you'd need to calculate it separately\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6cefc-0dd9-403e-bd11-5a7a57d4e92b",
   "metadata": {},
   "source": [
    "This example provides a foundational structure for how one might begin to approach building a Directed Information Neural Estimator in Julia using Flux. Remember, the actual implementation of DI estimation would require a more nuanced approach to both model design and loss function formulation, particularly to accurately capture and estimate directed information based on the principles of information theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359875f-47b5-49de-a52d-0d2e818fd743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
