{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7530f7cc",
   "metadata": {},
   "source": [
    "# Lab Session 3 - Federated Learning \n",
    "\n",
    "\n",
    "# Part 1: Federated Learning with non-IID data\n",
    "\n",
    "In this notebook, we study the detrimental effect of non-IID (non indepedent and identically distributed) data in the context of Federated Learning (FL).\n",
    "\n",
    "We consider a population of 10 FL devices that federate to train a Convolutional Neural Network (CNN) to classify E-MINST images. The E-MNIST dataset comprises handwritten digits and alphabet characters.\n",
    "\n",
    "We consider two scenarios with different degree of data heterogeneity across clients. \n",
    "\n",
    "In the first scenario, we consider a population of devices with heterogeneous label distributions (devices have a different fraction of samples from a specific class). \n",
    "In the second scenario, we also rotate the images of half of the participating devices to introduce an additional level of heterogeneity (covariate shift).\n",
    "\n",
    "In both cases, we train the CNN and measure its generalization performance along with useful metrics to understand the effect of data heterogeneity to the training dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d323593",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "This section contains all preliminaries classes and methods in order to simulate an FL procedure. In particular we will:\n",
    "1. Define methods to perform primitive operations such as copying model and averaging model updates.\n",
    "2. Define the local training and evaluating procedure that will be performed at every FL device.\n",
    "3. Define the federated learning device and server classes along with their necessary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb409031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd20355b",
   "metadata": {},
   "source": [
    "### Basic operations\n",
    "\n",
    "The following are shared methods that are used to perform basic operations on the model estimates, such as copying a source model to a target one, subtract a model to another, average model parameters and flattening model estimates into one dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f22d7854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy(target, source):\n",
    "    for name in target:\n",
    "        target[name].data = source[name].data.clone()\n",
    "    \n",
    "def subtract_(target, minuend, subtrahend):\n",
    "    for name in target:\n",
    "        target[name].data = minuend[name].data.clone()-subtrahend[name].data.clone()\n",
    "    \n",
    "def reduce_add_average(targets, sources):\n",
    "    for target in targets:\n",
    "        for name in target:\n",
    "            tmp = torch.mean(torch.stack([source[name].data for source in sources]), dim=0).clone()\n",
    "            target[name].data += tmp\n",
    "        \n",
    "def flatten(source):\n",
    "    return torch.cat([value.flatten() for value in source.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137d6403",
   "metadata": {},
   "source": [
    "### Training operator\n",
    "\n",
    "We now define the training operator to perform local training at each FL device. \n",
    "\n",
    "The methods takes as input: the current model, the data loader, an optimizer and a predefined number of local epochs.\n",
    "\n",
    "The methods updates the current model estimates, optimizing the loss function that is fed with samples from the data loader.\n",
    "\n",
    "**EXERCISE TODO: Define a proper classification loss function between the model prediction (y_hat) and the groundtruth (y)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a73e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op(model, loader, optimizer, epochs=1):\n",
    "    model.train()  \n",
    "    for ep in range(epochs):\n",
    "        running_loss, samples = 0.0, 0\n",
    "        for x, y in loader: \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat=model(x)\n",
    "            ## TODO: Define the classification loss \n",
    "            ## Your code here\n",
    "            \n",
    "            running_loss += loss.item()*y.shape[0]\n",
    "            samples += y.shape[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return running_loss / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca42d2",
   "metadata": {},
   "source": [
    "### Evaluation operator\n",
    "The following method evaluates the performance of the local model on a testing set.\n",
    "\n",
    "It takes as input the current model and the test data loader. It returns the fraction of correct predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4affb664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_op(model, loader):\n",
    "    model.train()\n",
    "    samples, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            _, predicted = torch.max(y_hat.data, 1)\n",
    "            samples += y.shape[0]\n",
    "            # Compute the number of correct samples \n",
    "            correct += (predicted == y).sum().item() \n",
    "    return correct/samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a83b8",
   "metadata": {},
   "source": [
    "## Federated Learning Client\n",
    "We are now ready to define the FL client class. \n",
    "This class is used to simulate an FL device and its basic functionalities.\n",
    "\n",
    "The FL client comprises:\n",
    "1. A machine learning model to be optimized (self.model)\n",
    "2. The parameter of the model (self.W)\n",
    "3. An optimizer to update the parameter of the model (self.optimizer)\n",
    "4. Data (self.data) that is split into training data (self.train_loader) and evaluation data (self.eval_loader)\n",
    "5. The model updates (self.dW)\n",
    "\n",
    "The basic functionalities of an FL device are:\n",
    "1. Synchroniziation with the server, namely copying the server parameters to the current estimate (self.W)\n",
    "2. Compute the model update (self.dW) as the difference between the old model (self.W_old) and the locally trained one (self.W). \n",
    "At round $t$ the update at node $i$ is computed as \n",
    "$$\\Delta W_i\\leftarrow W_i^{t}-W_i^{t-1}$$  \n",
    "where $W_i^{t}$ is the locally updated model from $W_i^{t-1}$.\n",
    "3. Evaluate the local model on an evaluation set. \n",
    "\n",
    "\n",
    "**EXERCISE TODO: Using the baseline methods defined before, compute the model update (self.dW) starting the updated (self.W) and old model estimate (self.W_old).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cced743",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class Client(object):\n",
    "    def __init__(self, model_fn, optimizer_fn, data, idnum, batch_size=128, train_frac=0.8):\n",
    "        self.model = model_fn().to(device)\n",
    "        self.optimizer = optimizer_fn(self.model.parameters())\n",
    "        self.W = {key: value for key, value in self.model.named_parameters()}\n",
    "        self.data = data\n",
    "        n_train = int(len(data)*train_frac)\n",
    "        n_eval = len(data) - n_train \n",
    "        data_train, data_eval = torch.utils.data.random_split(self.data, [n_train, n_eval])\n",
    "        self.train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "        self.eval_loader = DataLoader(data_eval, batch_size=batch_size, shuffle=False)\n",
    "        self.id = idnum\n",
    "        self.dW = {key : torch.zeros_like(value) for key, value in self.model.named_parameters()}\n",
    "        self.W_old = {key : torch.zeros_like(value) for key, value in self.model.named_parameters()}\n",
    "        \n",
    "    def synchronize_with_server(self, server):\n",
    "        copy(target=self.W, source=server.W)\n",
    "    \n",
    "    def compute_weight_update(self, epochs=1, loader=None):\n",
    "        copy(target=self.W_old, source=self.W)\n",
    "        self.optimizer.param_groups[0][\"lr\"]*=0.99\n",
    "        train_stats = train_op(self.model, self.train_loader if not loader else loader, self.optimizer, epochs)\n",
    "        ##TODO: Compute the model update \n",
    "        ## Your code here\n",
    "        \n",
    "        return train_stats  \n",
    "\n",
    "    def reset(self): \n",
    "        copy(target=self.W, source=self.W_old)\n",
    "        \n",
    "    def evaluate(self, loader=None):\n",
    "        return eval_op(self.model, self.eval_loader if not loader else loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81dbba7",
   "metadata": {},
   "source": [
    "## Parameter Server (PS)\n",
    "The following class is used to simulate a parameter server and its basic functionalities.\n",
    "\n",
    "\n",
    "The basic functionalities of a PS devices are:\n",
    "1. Sampling FL devices for training (select_clients)\n",
    "2. Compute the aggregate model (aggregate_weight_updates)\n",
    "2. Evaluate the current model estimate (evaluate)\n",
    "\n",
    "We consider additional and non-essential functionalities to track the training dynamics, in particular\n",
    "1. Compute the magnitude of the largest update returned by the FL user $$\\max_i \\lVert \\Delta W_i\\lVert$$\n",
    "2. Compute the mean update vector  $$\\lVert \\sum_i \\Delta W_i\\lVert$$\n",
    "\n",
    "\n",
    "**EXERCISE TODO: Define the averaging rule inside the aggregate_weight_updates method using the reduce_add_average method defined above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf162995",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class Server(object):\n",
    "    def __init__(self, model_fn, data):\n",
    "        self.model = model_fn().to(device)\n",
    "        self.data = data\n",
    "        self.W = {key: value for key, value in self.model.named_parameters()}\n",
    "        self.loader = DataLoader(self.data, batch_size=128, shuffle=False)\n",
    "        self.model_cache = []\n",
    "    \n",
    "    def select_clients(self, clients, frac=1.0):\n",
    "        return random.sample(clients, int(len(clients)*frac)) \n",
    "\n",
    "    def aggregate_weight_updates(self, clients):\n",
    "        updates=[client.dW for client in clients]\n",
    "        local_models=[client.W for client in clients]\n",
    "        ## TODO: Implement the federated averaging rule\n",
    "        ##Your code here\n",
    "        ##\n",
    "                    \n",
    "    def compute_max_update_norm(self, cluster):\n",
    "        return np.max([torch.norm(flatten(client.dW)).item() for client in cluster])\n",
    "    \n",
    "    def compute_mean_update_norm(self, cluster):\n",
    "        return torch.norm(torch.mean(torch.stack([flatten(client.dW) for client in cluster]), dim=0)).item()\n",
    "\n",
    "    def cache_model(self, idcs, params, accuracies):\n",
    "        self.model_cache += [(idcs, \n",
    "                            {name : params[name].data.clone() for name in params},\n",
    "                            [accuracies[i] for i in idcs])]\n",
    "\n",
    "    def evaluate(self, loader=None):\n",
    "        return eval_op(self.model, self.eval_loader if not loader else loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d996b",
   "metadata": {},
   "source": [
    "# Federated Learning with Label Shift\n",
    "\n",
    "Having defined all necessary ingredients to instatiate the FL procedure, we now train a CNN to classify the E-MNIST data using a Federated Learning protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ad976c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconvnets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResNet\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# from helper import ExperimentLogger, display_train_stats\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_noniid, CustomSubset\n\u001b[1;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     20\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_utils'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from models import ConvNet\n",
    "# from convnets.models import ResNet\n",
    "from helper import ExperimentLogger, display_train_stats\n",
    "from data_utils import split_noniid, CustomSubset\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa4bc64",
   "metadata": {},
   "source": [
    "We simulate a scenario with 10 clients and distribute the data among the clients in a non-iid fashion accoding to a Dirichlet distribution parametrized with alpha=1.0, this allows to account for the label shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLIENTS = 10\n",
    "DIRICHLET_ALPHA = 1\n",
    "\n",
    "\n",
    "data = datasets.EMNIST(root=\".\", split=\"byclass\", download=True)\n",
    "\n",
    "mapp = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C',\n",
    "       'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P',\n",
    "       'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c',\n",
    "       'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "       'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'], dtype='<U1')\n",
    "\n",
    "idcs = np.random.permutation(len(data))\n",
    "train_idcs, test_idcs = idcs[:10000], idcs[10000:20000]\n",
    "train_labels = data.targets.numpy()\n",
    "\n",
    "client_idcs = split_noniid(train_idcs, train_labels, alpha=DIRICHLET_ALPHA, n_clients=N_CLIENTS)\n",
    "\n",
    "client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eeaf2c",
   "metadata": {},
   "source": [
    "The data distribution is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb365e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,3))\n",
    "plt.hist([train_labels[idc]for idc in client_idcs], stacked=True, \n",
    "         bins=np.arange(min(train_labels)-0.5, max(train_labels) + 1.5, 1),\n",
    "        label=[\"Client {}\".format(i) for i in range(N_CLIENTS)])\n",
    "plt.xticks(np.arange(62), mapp)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1bfaf3",
   "metadata": {},
   "source": [
    " We now instantiate a population of 10 clients and define the PS server. The clients use an SGD optimizer with a learning rate $\\eta=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585d7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, idnum=i) \n",
    "           for i, dat in enumerate(client_data)]\n",
    "server = Server(ConvNet, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for client in [clients[0], clients[5]]:\n",
    "    x, y = iter(client.train_loader).next()\n",
    "\n",
    "    print(\"Client {}:\".format(client.id))\n",
    "    plt.figure(figsize=(15,1))\n",
    "    for i in range(10):\n",
    "        plt.subplot(1,10,i+1)\n",
    "        plt.imshow(x[i,0].numpy().T, cmap=\"Greys\")\n",
    "        plt.title(\"Label: {}\".format(mapp[y[i].item()]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70094cc2",
   "metadata": {},
   "source": [
    "Everything is set up to the Federated Learning algorithm.\n",
    "\n",
    "During training, we track the mean and std client accuracies, as well as the average and maximum client update norms. These two metrics allow us to measure the level of heterogeneity within the population of devices. When the average client update is small, it means that the FL procedure is close to a stationary point; however, if the maximum client update norm remains large, this means that there exist clients that have not converged locally.\n",
    "\n",
    "\n",
    "**EXERCISE TODO: Using the methods above, define the training round of the FL procedure. In particular, sample a fraction of population of devices, compute the weights updates for each participating devices and aggregate the weights updates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93788d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMUNICATION_ROUNDS = 80\n",
    "\n",
    "    \n",
    "cfl_stats = ExperimentLogger()\n",
    "    \n",
    "cluster_indices = [np.arange(len(clients)).astype(\"int\")]\n",
    "\n",
    "\n",
    "for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "\n",
    "    if c_round == 1:\n",
    "        for client in clients:\n",
    "            client.synchronize_with_server(server)\n",
    "    ## TODO: Sample the participating clients\n",
    "    ## Your code here\n",
    "\n",
    "    for client in participating_clients:\n",
    "        ## TODO: Compute the update for each participating clients \n",
    "        ## Your code here\n",
    "\n",
    "    for idc in cluster_indices:\n",
    "        max_norm = server.compute_max_update_norm([clients[i] for i in idc])\n",
    "        mean_norm = server.compute_mean_update_norm([clients[i] for i in idc])\n",
    "\n",
    "    ## TODO: Aggregate the updates for each participating clients \n",
    "    ## Your code here\n",
    "\n",
    "    acc_clients = [client.evaluate() for client in clients]\n",
    "    \n",
    "    cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\n",
    "                  \"rounds\" : c_round, \"clusters\" : cluster_indices})\n",
    "    \n",
    "    \n",
    "    display_train_stats(cfl_stats,COMMUNICATION_ROUNDS)\n",
    "\n",
    "    \n",
    "for idc in cluster_indices:    \n",
    "    server.cache_model(idc, clients[idc[0]].W, acc_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dbae91",
   "metadata": {},
   "source": [
    "Even if the FL procedure converges (plateau on the left plot) and the mean model update $\\lVert\\sum_i\\Delta W_i\\lVert$ converges to zero, there exist clients with large model updates $\\max_i\\lVert\\Delta W_i\\lVert$. This indicates that there exists some degree of model heterogeneity.\n",
    "\n",
    "We now compute the local performance on each client of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros([10, len(server.model_cache)])\n",
    "for i, (idcs, W, accs) in enumerate(server.model_cache):\n",
    "    results[idcs, i] = np.array(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c0e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(results, columns=[\"FL Model\"]+[\"Model {}\".format(i) \n",
    "                                                    for i in range(results.shape[1]-1)],\n",
    "            index = [\"Client {}\".format(i) for i in range(results.shape[0])])\n",
    "\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "frame.T.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7b105",
   "metadata": {},
   "source": [
    "As we can see, the performance are heterogeneous and with average performance $\\sim 78\\%$. The difference between the best and the worst performing client is $\\sim 10\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558ec4e",
   "metadata": {},
   "source": [
    "# Federated Learning with Label Shift and Covariate Shift\n",
    "\n",
    "Now, we simulate a clustering structure in the client population, by rotating the data for half of the first 5 clients by 180 degree. We display 10 data samples from the 1st and the 6th client for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, client_datum in enumerate(client_data):\n",
    "    if i<5:\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.RandomRotation((180,180)),\n",
    "                                                      transforms.ToTensor()])\n",
    "    else:\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50503fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, idnum=i) \n",
    "           for i, dat in enumerate(client_data)]\n",
    "server = Server(ConvNet, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for client in [clients[0], clients[5]]:\n",
    "    x, y = iter(client.train_loader).next()\n",
    "\n",
    "    print(\"Client {}:\".format(client.id))\n",
    "    plt.figure(figsize=(15,1))\n",
    "    for i in range(10):\n",
    "        plt.subplot(1,10,i+1)\n",
    "        plt.imshow(x[i,0].numpy().T, cmap=\"Greys\")\n",
    "        plt.title(\"Label: {}\".format(mapp[y[i].item()]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aa0a5b",
   "metadata": {},
   "source": [
    "We run again the same FL procedure using this new data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb4505",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMUNICATION_ROUNDS = 80\n",
    "    \n",
    "cfl_stats = ExperimentLogger()\n",
    "    \n",
    "cluster_indices = [np.arange(len(clients)).astype(\"int\")]\n",
    "\n",
    "\n",
    "for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "\n",
    "    if c_round == 1:\n",
    "        for client in clients:\n",
    "            client.synchronize_with_server(server)\n",
    "            \n",
    "    participating_clients = server.select_clients(clients, frac=1.0)\n",
    "\n",
    "    for client in participating_clients:\n",
    "        train_stats = client.compute_weight_update(epochs=1)\n",
    "        client.reset()\n",
    "\n",
    "    for idc in cluster_indices:\n",
    "        max_norm = server.compute_max_update_norm([clients[i] for i in idc])\n",
    "        mean_norm = server.compute_mean_update_norm([clients[i] for i in idc])\n",
    "\n",
    "\n",
    "    server.aggregate_weight_updates(participating_clients)\n",
    "\n",
    "    acc_clients = [client.evaluate() for client in clients]\n",
    "    \n",
    "    cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\n",
    "                  \"rounds\" : c_round, \"clusters\" : cluster_indices})\n",
    "    \n",
    "    \n",
    "    display_train_stats(cfl_stats, COMMUNICATION_ROUNDS)\n",
    "\n",
    "    \n",
    "for idc in cluster_indices:    \n",
    "    server.cache_model(idc, clients[idc[0]].W, acc_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5b562",
   "metadata": {},
   "source": [
    "We can see that compared to the previous scenario, the performance greatly degrades ($\\sim 15\\%$). As expected, we record an even larger mismatch between the magnitude of the mean update magnitude and the maximum update magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb6528",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros([10, len(server.model_cache)])\n",
    "for i, (idcs, W, accs) in enumerate(server.model_cache):\n",
    "    results[idcs, i] = np.array(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(results, columns=[\"FL Model\"]+[\"Model {}\".format(i) \n",
    "                                                    for i in range(results.shape[1]-1)],\n",
    "            index = [\"Client {}\".format(i) for i in range(results.shape[0])])\n",
    "\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "frame.T.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5bdf76",
   "metadata": {},
   "source": [
    "The performance among different users remains heterogenous and in general lower than the previous case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9de99a",
   "metadata": {},
   "source": [
    "# Part 2: Clustered Federated Learning\n",
    "\n",
    "In the second part of the lab, we try to compensate for data heterogeneity using the Clustered FL algorithm proposed by Felix Sattler et al. (https://arxiv.org/abs/1910.01991)\n",
    "\n",
    "Clustered federated learning is a variant of the standard FL, in which similarity between clients updates is tracked during training and used to branch the FL procedure in independent streams each with a cluster of homogeneous users.\n",
    "\n",
    "Clustered FL employs the cosine score to measure similarity between users, namely given model $W_i\\in\\mathbb{R}^d$ and $W_j\\in\\mathbb{R}^d$, the cosine similarity is given by \n",
    "\n",
    "$$\\cos_s(W_i,W_j)=\\frac{W_i\\cdot W_j}{\\lVert W_i\\lVert\\lVert W_j\\lVert}$$\n",
    "\n",
    "**EXERCISE TO DO: implement the cosine similarity primitive between a list of source models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3966e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_angles(sources):\n",
    "    ## TODO: Implement the pairwise_angle function given by the following formula\n",
    "    angles = torch.zeros([len(sources), len(sources)])\n",
    "    for i, source1 in enumerate(sources):\n",
    "        for j, source2 in enumerate(sources):\n",
    "            s1 = flatten(source1)\n",
    "            s2 = flatten(source2)\n",
    "            ## TODO: Compute the cosine similarity score between the model param vectors s1 and s2\n",
    "            ## Your code here\n",
    "\n",
    "    return angles.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb7718",
   "metadata": {},
   "source": [
    "In the Clustered FL procedure, the parameter server, in addition to aggregating model estimates, also tracks similarity between users and cluster them based on their instantaneous similarity scores. Accordingly the Server class comprises three new methods:\n",
    "1. compute_pairwise_similarities that computes the similarity scores between the model updates returned by the FL users.\n",
    "2. cluster_clients that clusters the FL clients based on their similarity scores.\n",
    "3. aggregate_clusterwise, the modified aggregation rule that averages the model updates based on the current clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server(object):\n",
    "    def __init__(self, model_fn, data):\n",
    "        self.model = model_fn().to(device)\n",
    "        self.data = data\n",
    "        self.W = {key: value for key, value in self.model.named_parameters()}\n",
    "        self.loader = DataLoader(self.data, batch_size=128, shuffle=False)\n",
    "        self.model_cache = []\n",
    "    \n",
    "    def select_clients(self, clients, frac=1.0):\n",
    "        return random.sample(clients, int(len(clients)*frac)) \n",
    "    \n",
    "    def aggregate_weight_updates(self, clients):\n",
    "        # EXERCISE: Implement the federated averaging rule\n",
    "        reduce_add_average(target=self.W, sources=[client.dW for client in clients])\n",
    "        \n",
    "    def compute_pairwise_similarities(self, clients):\n",
    "        return pairwise_angles([client.dW for client in clients])\n",
    "  \n",
    "    def cluster_clients(self, S):\n",
    "        clustering = AgglomerativeClustering(affinity=\"precomputed\", linkage=\"complete\").fit(-S)\n",
    "\n",
    "        c1 = np.argwhere(clustering.labels_ == 0).flatten() \n",
    "        c2 = np.argwhere(clustering.labels_ == 1).flatten() \n",
    "        return c1, c2\n",
    "    \n",
    "    def aggregate_clusterwise(self, client_clusters):\n",
    "        for cluster in client_clusters:\n",
    "            reduce_add_average(targets=[client.W for client in cluster], \n",
    "                               sources=[client.dW for client in cluster])\n",
    "            \n",
    "    def compute_max_update_norm(self, cluster):\n",
    "        return np.max([torch.norm(flatten(client.dW)).item() for client in cluster])\n",
    "    \n",
    "    def compute_mean_update_norm(self, cluster):\n",
    "        return torch.norm(torch.mean(torch.stack([flatten(client.dW) for client in cluster]), dim=0)).item()\n",
    "\n",
    "    def cache_model(self, idcs, params, accuracies):\n",
    "        self.model_cache += [(idcs, \n",
    "                            {name : params[name].data.clone() for name in params},\n",
    "                            [accuracies[i] for i in idcs])]\n",
    "\n",
    "    def evaluate(self, loader=None):\n",
    "        return eval_op(self.model, self.eval_loader if not loader else loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015905d2",
   "metadata": {},
   "source": [
    "We are now ready to run the Clustered FL procedure. In particular, the procedure works iteratively as the standard FL procedure, but at every round it measures the pairwise similarity. If the procedure converges $\\lVert\\sum_i\\Delta W_i\\lVert \\leq \\epsilon_1$ and there exist clients with large model updates $\\max_i\\lVert\\Delta W_i\\lVert >\\epsilon_2$, then it performs clustering of the users based on their similarity.\n",
    "\n",
    "**EXERCISE TODO: define the condition so as to perform clustering of the clients.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaedb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, idnum=i) \n",
    "           for i, dat in enumerate(client_data)]\n",
    "server = Server(ConvNet, test_data)\n",
    "\n",
    "COMMUNICATION_ROUNDS = 80\n",
    "EPS_1 = 0.4\n",
    "EPS_2 = 1.6\n",
    "    \n",
    "    \n",
    "cfl_stats = ExperimentLogger()\n",
    "    \n",
    "cluster_indices = [np.arange(len(clients)).astype(\"int\")]\n",
    "client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "\n",
    "for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "\n",
    "    if c_round == 1:\n",
    "        for client in clients:\n",
    "            client.synchronize_with_server(server)\n",
    "            \n",
    "    participating_clients = server.select_clients(clients, frac=1.0)\n",
    "\n",
    "    for client in participating_clients:\n",
    "        train_stats = client.compute_weight_update(epochs=1)\n",
    "        client.reset()\n",
    "\n",
    "    similarities = server.compute_pairwise_similarities(clients)\n",
    "\n",
    "    cluster_indices_new = []\n",
    "    for idc in cluster_indices:\n",
    "        max_norm = server.compute_max_update_norm([clients[i] for i in idc])\n",
    "        mean_norm = server.compute_mean_update_norm([clients[i] for i in idc])\n",
    "        ## TODO: Define the condition for the clustering to take place\n",
    "        ## Your code here\n",
    "            \n",
    "            server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "            \n",
    "            c1, c2 = server.cluster_clients(similarities[idc][:,idc]) \n",
    "            cluster_indices_new += [c1, c2]\n",
    "             \n",
    "            cfl_stats.log({\"split\" : c_round})\n",
    "\n",
    "        else:\n",
    "            cluster_indices_new += [idc]\n",
    "        \n",
    "        \n",
    "    cluster_indices = cluster_indices_new\n",
    "    client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "    server.aggregate_clusterwise(client_clusters)\n",
    "\n",
    "    acc_clients = [client.evaluate() for client in clients]\n",
    "    \n",
    "    cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\n",
    "                  \"rounds\" : c_round, \"clusters\" : cluster_indices})\n",
    "    \n",
    "    \n",
    "    display_train_stats(cfl_stats,COMMUNICATION_ROUNDS, EPS_1, EPS_2)\n",
    "\n",
    "    \n",
    "for idc in cluster_indices:    \n",
    "    server.cache_model(idc, clients[idc[0]].W, acc_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6bf4ed",
   "metadata": {},
   "source": [
    "Around Round 30, Clustered FL splits the group of 10 users into two smaller groups of 5 users, each corresponding to the different rotation of the E-MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6322c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros([10, len(server.model_cache)])\n",
    "for i, (idcs, W, accs) in enumerate(server.model_cache):\n",
    "    results[idcs, i] = np.array(accs)\n",
    "\n",
    "frame = pd.DataFrame(results, columns=[\"FL Model\"]+[\"Model {}\".format(i) \n",
    "                                                    for i in range(results.shape[1]-1)],\n",
    "            index = [\"Client {}\".format(i) for i in range(results.shape[0])])\n",
    "\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "frame.T.style.apply(highlight_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74d6b0",
   "metadata": {},
   "source": [
    "At the end of training there exist two models (one for each cluster) and as we can see, each model is personalized for the different rotation of the E-MNIST samples. \n",
    "\n",
    "**EXERCISE TODO: Compare the performance with the previous procedure without clustering.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3493f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
