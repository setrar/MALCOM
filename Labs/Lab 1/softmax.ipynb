{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b0878c-b0d5-44e3-a7d2-bf093090d986",
   "metadata": {},
   "source": [
    "# The softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02f92e-df23-4b0d-9aa0-800255fc7896",
   "metadata": {},
   "source": [
    "The softmax function is widely used in machine learning, particularly in classification tasks. Here’s a detailed look at its usage and importance:\n",
    "\n",
    "### Definition\n",
    "\n",
    "The softmax function takes an $n$-dimensional vector of real numbers and transforms it into a probability distribution of $n$ possible outcomes. Each component of the resulting vector is between 0 and 1, and the sum of all components is 1. The formula for the softmax function for a vector $ \\mathbf{z} $ is:\n",
    "\n",
    "$ \\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}} $\n",
    "\n",
    "### Usage\n",
    "\n",
    "1. **Multi-class Classification**:\n",
    "   - **Output Layer of Neural Networks**: The most common use of the softmax function is in the output layer of neural networks for multi-class classification problems. It converts the raw output scores (logits) into probabilities, which are easier to interpret and work with.\n",
    "   - **Probability Interpretation**: The output probabilities indicate the likelihood of each class. The class with the highest probability is typically chosen as the predicted class.\n",
    "\n",
    "2. **Probability Distribution**:\n",
    "   - **Normalization**: Softmax normalizes the logits, ensuring they sum to 1. This property is crucial for tasks that require a valid probability distribution over classes.\n",
    "\n",
    "3. **Cross-Entropy Loss**:\n",
    "   - **Training Stability**: When combined with the cross-entropy loss function, softmax provides a smooth gradient, which helps in the efficient training of neural networks using gradient-based optimization methods.\n",
    "\n",
    "### Why Softmax is Useful\n",
    "\n",
    "1. **Interpretable Outputs**:\n",
    "   - The softmax function transforms the raw model outputs into a probability distribution, making the outputs interpretable as probabilities.\n",
    "\n",
    "2. **Gradient Computation**:\n",
    "   - Softmax, when used with cross-entropy loss, produces well-behaved gradients, which facilitate efficient learning during backpropagation.\n",
    "\n",
    "3. **Handling Multi-class Problems**:\n",
    "   - Unlike binary classification where a single sigmoid function suffices, softmax can handle multiple classes in a single pass by providing a distribution over all possible classes.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a neural network model for digit classification (0-9). The network's final layer might output a vector of logits $[z_0, z_1, \\ldots, z_9]$. Applying softmax to this vector will yield:\n",
    "\n",
    "$ \\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=0}^{9} e^{z_j}} $\n",
    "\n",
    "This transforms the logits into probabilities, such as:\n",
    "\n",
    "$ [0.1, 0.05, 0.7, 0.05, 0.02, 0.02, 0.03, 0.01, 0.01, 0.01] $\n",
    "\n",
    "Here, the network predicts that the input digit is most likely '2' with a probability of 0.7.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The softmax function is a crucial component in neural networks, particularly for multi-class classification tasks. It transforms logits into interpretable probabilities, enabling the network to make meaningful predictions and facilitating effective training with smooth gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a8ed3f-fda8-4c66-9928-23ee25b0aa60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum (generic function with 23 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "∑ = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95364676-5eb3-4ce7-9c4d-c8d95c1ea3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the softmax function\n",
    "function softmax(z) # z -> logits\n",
    "    return exp.(z) ./ ∑(exp.(z))  # Normalize to get probabilities\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4aaf1e-35b0-479a-a1db-a73f55f03cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: [2.0, 1.0, 0.1, -0.5, 0.5, 2.5, -1.0, 0.0, 1.5, -0.8]\n",
      "Probabilities: [0.2312754983247151, 0.08508150108034303, 0.03459155694445449, 0.018984248961725746, 0.0516045389796016, 0.38130883347972966, 0.011514529046904394, 0.03129973507146406, 0.1402756805742575, 0.014063877536804418]\n",
      "Sum of probabilities: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Example logits from a neural network output layer (e.g., for digit classification 0-9)\n",
    "logits = [2.0, 1.0, 0.1, -0.5, 0.5, 2.5, -1.0, 0.0, 1.5, -0.8];\n",
    "\n",
    "# Apply the softmax function to the logits\n",
    "probabilities = softmax(logits);\n",
    "\n",
    "# Print the probabilities\n",
    "println(\"Logits: \", logits)\n",
    "println(\"Probabilities: \", probabilities)\n",
    "\n",
    "# Verify that the probabilities sum to 1\n",
    "println(\"Sum of probabilities: \", ∑(probabilities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9728d4f9-d5a7-4c4b-8219-33f05450d2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(20 => 10, σ),                   \u001b[90m# 210 parameters\u001b[39m\n",
       "  Dense(10 => 10),                      \u001b[90m# 110 parameters\u001b[39m\n",
       "  softmax,\n",
       ") \u001b[90m                  # Total: 4 arrays, \u001b[39m320 parameters, 1.500 KiB."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example neural network using Flux\n",
    "# Define a simple model with one hidden layer and softmax output\n",
    "model = Chain(\n",
    "    Dense(20, 10, sigmoid),  # Input layer: 20 features, hidden layer: 10 units\n",
    "    Dense(10, 10),           # Output layer: 10 classes\n",
    "    softmax                  # Apply softmax to the output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d08b54d-feb6-483a-9ede-7175867f520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output probabilities: Float32"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mLayer with Float32 parameters got Float64 input.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  The input will be converted, but any earlier layers may be very slow.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  layer = Dense(20 => 10, σ)  \u001b[90m# 210 parameters\u001b[39m\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39m  summary(x) = \"20-element Vector{Float64}\"\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Flux ~/.julia/packages/Flux/Wz6D4/src/layers/stateless.jl:60\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.060023494, 0.090147905, 0.12973249, 0.080260485, 0.050536975, 0.07160723, 0.08341373, 0.13941053, 0.051136695, 0.24373041]\n",
      "Sum of output probabilities: 0.99999994\n"
     ]
    }
   ],
   "source": [
    "# Example input vector (20-dimensional)\n",
    "input_vector = rand(20)\n",
    "\n",
    "# Perform a forward pass through the network\n",
    "output_probabilities = model(input_vector)\n",
    "\n",
    "# Print the output probabilities\n",
    "println(\"Output probabilities: \", output_probabilities)\n",
    "\n",
    "# Verify that the output probabilities sum to 1\n",
    "println(\"Sum of output probabilities: \", sum(output_probabilities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca3b655-e186-4303-a5ab-9b6bac5ca7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of defining a loss function using cross-entropy\n",
    "loss(x, y) = Flux.crossentropy(model(x), y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ebee26d-94bb-47db-abf8-91942a9bb38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×100 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  …  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  1  1  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  1  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1\n",
       " ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  1  ⋅  …  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  1  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅\n",
       " 1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅\n",
       " ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅     1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate example data\n",
    "X = rand(20, 100)  # 100 samples of 20-dimensional input\n",
    "Y = Flux.onehotbatch(rand(1:10, 100), 1:10)  # 100 one-hot encoded labels for 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57f12cbe-0730-41a4-a776-7eb30acf7085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using stochastic gradient descent\n",
    "opt = Descent(0.01)\n",
    "Flux.train!(loss, Flux.params(model), [(X, Y)], opt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79b13bfd-e150-40b9-bb8b-13beeb606840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Float32[0.06021196, 0.09020385, 0.1295892, 0.080533534, 0.050752405, 0.07192128, 0.08346487, 0.1394814, 0.05117373, 0.24266773]\n"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "prediction = model(input_vector)\n",
    "println(\"Prediction: \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305e82f-a352-4bfa-b453-b50abd320917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
