{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139573b8-e499-4b91-979c-0b2a516c3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "using Flux, MLDatasets, Statistics, Random, BSON\n",
    "using Flux.Optimise: update!\n",
    "using Flux: logitbinarycrossentropy, binarycrossentropy, onecold, DataLoader, gradient, setup\n",
    "using Metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82cd86b4-662d-47f9-8b8f-b39f100fa0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mUsing backend: Metal.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(::Flux.FluxMetalDevice) (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Metal and set the current device\n",
    "# metal_dir = dirname(@__DIR__)\n",
    "# Pkg.activate(; temp=true)\n",
    "# Pkg.add([\"Metal\", \"Flux\", \"DataFrames\", \"OneHotArrays\"])\n",
    "# Pkg.develop(path=metal_dir)\n",
    "# using Metal\n",
    "device = Flux.get_device(; verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "582e38ef-d688-4991-bde4-679d53aebbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "batchSize, latentDim = 500, 100\n",
    "epochs = 40\n",
    "etaD, etaG = 0.0002, 0.0002;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c7875f3-0787-4adf-a89c-5343cc666f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mThe CUDA functionality is being called but\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m`CUDA.jl` must be loaded to access it.\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39mAdd `using CUDA` or `import CUDA` to your code.\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "images, _ = MNIST(split=:train)[:]\n",
    "images = reshape(@.(2f0 * images - 1f0), 28, 28, 1, :)\n",
    "images = gpu(images);  # Move images to GPU early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053e466f-d152-4757-88dd-841d9b7f1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageTensor = reshape(@.(2f0 * images - 1f0), 28, 28, 1, :)\n",
    "data = [imageTensor[:, :, :, r] for r in Iterators.partition(1:60000, batchSize)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac7df6f-a359-4d65-9c5d-3f9cc479d413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "  Dense(100 => 12544),                  \u001b[90m# 1_266_944 parameters\u001b[39m\n",
       "  BatchNorm(12544, relu),               \u001b[90m# 25_088 parameters\u001b[39m\u001b[90m, plus 25_088\u001b[39m\n",
       "  var\"#9#10\"(),\n",
       "  ConvTranspose((4, 4), 256 => 128, pad=1, stride=2),  \u001b[90m# 524_416 parameters\u001b[39m\n",
       "  BatchNorm(128, relu),                 \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 256\u001b[39m\n",
       "  ConvTranspose((4, 4), 128 => 64, pad=1, stride=2),  \u001b[90m# 131_136 parameters\u001b[39m\n",
       "  BatchNorm(64, relu),                  \u001b[90m# 128 parameters\u001b[39m\u001b[90m, plus 128\u001b[39m\n",
       "  ConvTranspose((4, 4), 64 => 1, tanh, pad=1, stride=2),  \u001b[90m# 1_025 parameters\u001b[39m\n",
       ") \u001b[90m        # Total: 14 trainable arrays, \u001b[39m1_948_993 parameters,\n",
       "\u001b[90m          # plus 6 non-trainable, 25_472 parameters, summarysize \u001b[39m7.534 MiB."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Definitions with CPU initialization\n",
    "dscr = Chain(\n",
    "    Conv((4,4), 1=>64, stride=2, pad=1),\n",
    "    x -> leakyrelu.(x, 0.2),\n",
    "    Dropout(0.25),\n",
    "    Conv((4,4), 64=>128, stride=2, pad=1),\n",
    "    x -> leakyrelu.(x, 0.2),\n",
    "    Dropout(0.25),\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(7*7*128, 1)\n",
    ") |> gpu  # Move the entire model to GPU after creation\n",
    "\n",
    "gen = Chain(\n",
    "    Dense(latentDim, 7*7*256),\n",
    "    BatchNorm(7*7*256, relu),\n",
    "    x -> reshape(x, 7, 7, 256, :),\n",
    "    ConvTranspose((4,4), 256=>128, stride=2, pad=1),\n",
    "    BatchNorm(128, relu),\n",
    "    ConvTranspose((4,4), 128=>64, stride=2, pad=1),\n",
    "    BatchNorm(64, relu),\n",
    "    ConvTranspose((4,4), 64=>1, tanh, stride=2, pad=1)\n",
    ") |> gpu  # Move the entire model to GPU after creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ed1f47-4203-4444-9150-ec5456ea2971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gLoss (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss Functions\n",
    "dLoss(realOut, fakeOut) = mean(logitbinarycrossentropy.(realOut, 1f0)) +\n",
    "                          mean(logitbinarycrossentropy.(fakeOut, 0f0))\n",
    "gLoss(u) = mean(logitbinarycrossentropy.(u, 1f0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9fd5a23-6b7e-4c3a-ae01-92e67bdeb4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateD! (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update Functions\n",
    "function updateD!(gen, dscr, x, opt_dscr)\n",
    "    noise = randn!(similar(x, (latentDim, batchSize)))\n",
    "    fakeInput = gen(noise)\n",
    "    ps = Flux.params(dscr)\n",
    "    loss, back = Flux.pullback(()->dLoss(dscr(x), dscr(fakeInput)), ps)\n",
    "    grad = back(1f0)\n",
    "    update!(opt_dscr, ps, grad)\n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95568b8-85f1-46b6-8e68-73f115cde241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updateG! (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function updateG!(gen, dscr, x, optGen)\n",
    "    noise = randn!(similar(x, (latentDim, batchSize)))\n",
    "    ps = Flux.params(gen)\n",
    "    loss, back = Flux.pullback(()->gLoss(dscr(gen(noise))), ps)\n",
    "    grad = back(1f0)\n",
    "    update!(optGen, ps, grad)\n",
    "    return loss\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "858b6f3c-08d3-4c5d-b7eb-ad41aeff4743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Adam(0.0002, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}()), Adam(0.0002, (0.9, 0.999), 1.0e-8, IdDict{Any, Any}()))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimization\n",
    "optDscr, optGen = ADAM(etaD), ADAM(etaG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1400d18-dc98-4c6a-b8cf-a9ee473d02a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: cannot take the CPU address of a MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate}",
     "output_type": "error",
     "traceback": [
      "ArgumentError: cannot take the CPU address of a MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate}",
      "",
      "Stacktrace:",
      "  [1] unsafe_convert(::Type{Ptr{Float32}}, x::MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate})",
      "    @ Metal ~/.julia/packages/Metal/lnkVP/src/array.jl:148",
      "  [2] gemm!(transA::Char, transB::Char, alpha::Float32, A::Matrix{Float32}, B::MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate}, beta::Float32, C::MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate})",
      "    @ LinearAlgebra.BLAS ~/.julia/juliaup/julia-1.10.2+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/LinearAlgebra/src/blas.jl:1524",
      "  [3] gemm_wrapper!(C::MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate}, tA::Char, tB::Char, A::Matrix{Float32}, B::MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate}, _add::LinearAlgebra.MulAddMul{true, true, Bool, Bool})",
      "    @ LinearAlgebra ~/.julia/juliaup/julia-1.10.2+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:605",
      "  [4] generic_matmatmul!(C::MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate}, tA::Char, tB::Char, A::Matrix{Float32}, B::MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate}, _add::LinearAlgebra.MulAddMul{true, true, Bool, Bool})",
      "    @ LinearAlgebra ~/.julia/juliaup/julia-1.10.2+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:352",
      "  [5] mul!",
      "    @ ~/.julia/juliaup/julia-1.10.2+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:263 [inlined]",
      "  [6] mul!",
      "    @ ~/.julia/juliaup/julia-1.10.2+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:237 [inlined]",
      "  [7] *(A::Matrix{Float32}, B::MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate})",
      "    @ LinearAlgebra ~/.julia/juliaup/julia-1.10.2+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/LinearAlgebra/src/matmul.jl:113",
      "  [8] (::Dense{typeof(identity), Matrix{Float32}, Vector{Float32}})(x::MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate})",
      "    @ Flux ~/.julia/packages/Flux/Wz6D4/src/layers/basic.jl:174",
      "  [9] macro expansion",
      "    @ ~/.julia/packages/Flux/Wz6D4/src/layers/basic.jl:53 [inlined]",
      " [10] _applychain(layers::Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, BatchNorm{typeof(relu), Vector{Float32}, Float32, Vector{Float32}}, var\"#9#10\", ConvTranspose{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(relu), Vector{Float32}, Float32, Vector{Float32}}, ConvTranspose{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(relu), Vector{Float32}, Float32, Vector{Float32}}, ConvTranspose{2, 4, typeof(tanh), Array{Float32, 4}, Vector{Float32}}}, x::MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate})",
      "    @ Flux ~/.julia/packages/Flux/Wz6D4/src/layers/basic.jl:53",
      " [11] (::Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, BatchNorm{typeof(relu), Vector{Float32}, Float32, Vector{Float32}}, var\"#9#10\", ConvTranspose{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(relu), Vector{Float32}, Float32, Vector{Float32}}, ConvTranspose{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(relu), Vector{Float32}, Float32, Vector{Float32}}, ConvTranspose{2, 4, typeof(tanh), Array{Float32, 4}, Vector{Float32}}}})(x::MtlMatrix{Float32, Metal.MTL.MTLResourceStorageModePrivate})",
      "    @ Flux ~/.julia/packages/Flux/Wz6D4/src/layers/basic.jl:51",
      " [12] updateD!(gen::Chain{Tuple{Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}, BatchNorm{typeof(relu), Vector{Float32}, Float32, Vector{Float32}}, var\"#9#10\", ConvTranspose{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(relu), Vector{Float32}, Float32, Vector{Float32}}, ConvTranspose{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, BatchNorm{typeof(relu), Vector{Float32}, Float32, Vector{Float32}}, ConvTranspose{2, 4, typeof(tanh), Array{Float32, 4}, Vector{Float32}}}}, dscr::Chain{Tuple{Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, var\"#3#6\", Dropout{Float64, Colon, TaskLocalRNG}, Conv{2, 4, typeof(identity), Array{Float32, 4}, Vector{Float32}}, var\"#4#7\", Dropout{Float64, Colon, TaskLocalRNG}, var\"#5#8\", Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, x::MtlArray{Float32, 4, Metal.MTL.MTLResourceStorageModePrivate}, opt_dscr::Adam)",
      "    @ Main ./In[8]:4",
      " [13] macro expansion",
      "    @ ./In[11]:7 [inlined]",
      " [14] macro expansion",
      "    @ ./timing.jl:279 [inlined]",
      " [15] top-level scope",
      "    @ ./In[11]:3"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "cd(@__DIR__)\n",
    "@time begin\n",
    "    for ep in 1:epochs\n",
    "        for (bi, x) in enumerate(data)\n",
    "            x_gpu = x |> device  # Ensure the batch is moved to GPU\n",
    "            lossD = updateD!(gen, dscr, x_gpu, optDscr)\n",
    "            lossG = updateG!(gen, dscr, x_gpu, optGen)\n",
    "            @info \"Epoch $ep, batch $bi, D loss = $(lossD), G loss = $(lossG)\"\n",
    "        end\n",
    "        @info \"Saving generator for epoch $ep\"\n",
    "        BSON.@save \"../data/mnistGAN$(ep)_gpu.bson\" genParams=cpu.(params(gen))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d5e13-612c-4d4d-8551-873c027050d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
