{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73cbb831-b7f9-4bb4-b2cd-97431f69eee8",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x31;)** \n",
    "\n",
    "Why do the layers in a deep neural network architecture need to be non-linear? In other words, why linear layer is not desirable in neural nets?\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d72197-5a1d-42bd-add5-75213d1a338b",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Because linear functions are closed under composition, this is equivalent to having a single layer. Therefore, no matter how many layers exist, the network can only learn linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a1278-e10a-4b86-8a9f-a63d5a455c90",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x32;)** \n",
    "\n",
    "You are solving a binary classification task for a wifi modulation signal problem. The final two layers in your network are a ReLU activation followed by a sigmoid activation. What will happen?\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3cec5-abf7-47c1-96c0-fd4b65d5b535",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Using ReLU then sigmoid will cause all predictions to be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a466f-bb71-4d4c-8320-bcbe7d96be97",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x33;)** \n",
    "\n",
    "Softmax takes in an n-dimensional vector $x$  and outputs another n-dimensional vector $y$:\n",
    "\n",
    "$\n",
    "y = \\frac{e^{x_i}}{\\sum_k e^{x_k}}\n",
    "$\n",
    "\n",
    "The objective is to compute the gradient of $y$ w.r.t $x$. Let $\\delta_{ij} = \\frac{\\partial y_i}{\\partial y_j}$ \n",
    "\n",
    "Derive an expression for $(i) \\delta_{ii}$, $(ii) \\delta_{i,j}$ when $i \\neq j$\n",
    "\n",
    "Hint: Quotient rule of calculus. Let $h(x) = \\frac{f(x)}{g(x)}$ then $\n",
    "\\frac{\\partial h}{\\partial x} = \\frac{\\frac{\\partial f(x)}{\\partial x} * g(x) - \\frac{\\partial g(x)}{\\partial x} * f(x)}{ (g(x))^2}\n",
    "$\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8541e7c3-cfdb-4596-899e-2f8f1a281d8a",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Let's denote $f(x)= e^{x_i}$; $g(x) = \\sum e^{x_k}$ \n",
    "\n",
    "- [ ]  When $ i = j $:\n",
    "\n",
    "$$\n",
    "\\begin{flalign*}\n",
    "\\delta_{ii} &= \\frac{\\partial y_i}{\\partial x_i} = \\frac{\\frac{\\partial f(x)}{\\partial x} g(x) - \\frac{\\partial g(x)}{\\partial x} * f(x)}{ (g(x))^2} \\\\\n",
    "&= \\frac{e^{x_i} \\displaystyle\\sum_k e^{x_k} - e^{x_i} e^{x_i}}{(\\displaystyle\\sum_k e^{x_k})^2} \\\\\n",
    "&= \\frac{e^{x_i} ( \\displaystyle\\sum_k e^{x_k} - e^{x_i}) }{(\\displaystyle\\sum_k e^{x_k})^2} \\\\\n",
    "&= y (1 - y)\n",
    "\\end{flalign*}\n",
    "$$\n",
    "\n",
    "- [ ] When $ i \\neq j $:\n",
    "\n",
    "$$\n",
    "\\begin{flalign*}\n",
    "\\delta_{ij} &= \\frac{\\partial y_i}{\\partial x_j} = \\frac{0 - e^{x_i} e^{x_j}}{(\\displaystyle\\sum e^{x_k})^2} - \\frac{e^{x_i} e^{x_j}}{\\displaystyle\\sum e^{x_i} \\displaystyle\\sum e^{x_j}} \\\\\n",
    "&= -y y_j\n",
    "\\end{flalign*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950e4a2-1972-480c-9339-a341443f784e",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x34;)** \n",
    "\n",
    "Let $s_k$ be the score for a specific class $k$ and $\\theta$ is a constant that we substract from all scores of a sample. Swho that $\\text{softmax}(s_k)$ is equal to $\\text{softmax}(s_k - \\theta)$.\n",
    "\n",
    "What does this property of the softmax function implies? Would you consider this property useful when training neural nets?\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d948bb-7795-468f-a889-4a2778aed9f2",
   "metadata": {},
   "source": [
    "Let's start by showing that $\\text{softmax}(s_k)$ is equal to $\\text{softmax}(s_k - \\theta)$.\n",
    "\n",
    "### Property Proof\n",
    "Given the softmax function for a vector of scores $ \\mathbf{s} $:\n",
    "$\n",
    "\\text{softmax}(s_k) = \\frac{e^{s_k}}{\\sum_{j=1}^{n} e^{s_j}}\n",
    "$\n",
    "\n",
    "Now, if we subtract a constant $\\theta$ from all scores, we get a new vector $ \\mathbf{s'} $ where $ s'_i = s_i - \\theta $ for all $ i $. The softmax function applied to this new vector is:\n",
    "$\n",
    "\\text{softmax}(s'_k) = \\frac{e^{s_k - \\theta}}{\\sum_{j=1}^{n} e^{s_j - \\theta}}\n",
    "$\n",
    "\n",
    "We can simplify the numerator and denominator:\n",
    "$\n",
    "\\text{softmax}(s'_k) = \\frac{e^{s_k - \\theta}}{\\sum_{j=1}^{n} e^{s_j - \\theta}} = \\frac{e^{s_k} \\cdot e^{-\\theta}}{\\sum_{j=1}^{n} e^{s_j} \\cdot e^{-\\theta}}\n",
    "$\n",
    "\n",
    "Since $ e^{-\\theta} $ is a constant and can be factored out of the sum in the denominator:\n",
    "$\n",
    "\\text{softmax}(s'_k) = \\frac{e^{s_k} \\cdot e^{-\\theta}}{e^{-\\theta} \\cdot \\sum_{j=1}^{n} e^{s_j}} = \\frac{e^{s_k}}{\\sum_{j=1}^{n} e^{s_j}}\n",
    "$\n",
    "\n",
    "Therefore,\n",
    "$\n",
    "\\boxed { \\text{softmax}(s'_k) = \\text{softmax}(s_k) }\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57941e16-9039-46e0-a022-c5a21d4cd950",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x35;)** \n",
    "\n",
    "You design a fully connected neural net architecture for an end-toend communication system, where all functions are sigmoid. You initilize the weigths with large positive numbers. Is this a good idea? Explain\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b307ff-a9a6-473b-ac72-3c5522200633",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Large weigths assumes $w_x$ to be large. When $w_x$ is large the gradient is small for sigmoid function. Hence, we encounter the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4effc529-54ae-4cf0-9b0e-fd10912b36a7",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x36;)** \n",
    "\n",
    "We would like to implement a deep wireless transmitter by training a fully connected neural net with 5 hidden layers, each with 10 hidden units. The input is a 20 dimensional vector and the output is a scalar. Calculate the total number of trainable parameters.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92ee01-ad9a-4369-9236-9faac1746440",
   "metadata": {},
   "source": [
    "Let's represent the calculation in a compact form. Given:\n",
    "\n",
    "- Input dimension: $ 20 $\n",
    "- Hidden layers: $ 5 $ hidden layers, each with $ 10 $ units\n",
    "- Output dimension: $ 1 $\n",
    "\n",
    "The total number of trainable parameters includes the weights and biases for each layer.\n",
    "\n",
    "### Layers Calculation\n",
    "\n",
    "1. **Input Layer to Hidden Layer 1:**\n",
    "   - Weights: $ 20 \\times 10 $\n",
    "   - Biases: $ 10 $\n",
    "\n",
    "2. **Hidden Layer $ i $ to Hidden Layer $ i+1 $ (for $ i = 1 $ to $ 4 $):**\n",
    "   - Weights: $ 10 \\times 10 $\n",
    "   - Biases: $ 10 $\n",
    "\n",
    "3. **Hidden Layer 5 to Output Layer:**\n",
    "   - Weights: $ 10 \\times 1 $\n",
    "   - Biases: $ 1 $\n",
    "\n",
    "### Total Calculation\n",
    "\n",
    "Let's sum these up:\n",
    "\n",
    "1. **Input to Hidden Layer 1:**\n",
    "   $\n",
    "   20 \\times 10 + 10 = 200 + 10 = 210\n",
    "   $\n",
    "\n",
    "2. **Hidden Layer 1 to Hidden Layer 2:**\n",
    "   $\n",
    "   10 \\times 10 + 10 = 100 + 10 = 110\n",
    "   $\n",
    "\n",
    "3. **Hidden Layer 2 to Hidden Layer 3:**\n",
    "   $\n",
    "   10 \\times 10 + 10 = 100 + 10 = 110\n",
    "   $\n",
    "\n",
    "4. **Hidden Layer 3 to Hidden Layer 4:**\n",
    "   $\n",
    "   10 \\times 10 + 10 = 100 + 10 = 110\n",
    "   $\n",
    "\n",
    "5. **Hidden Layer 4 to Hidden Layer 5:**\n",
    "   $\n",
    "   10 \\times 10 + 10 = 100 + 10 = 110\n",
    "   $\n",
    "\n",
    "6. **Hidden Layer 5 to Output Layer:**\n",
    "   $\n",
    "   10 \\times 1 + 1 = 10 + 1 = 11\n",
    "   $\n",
    "\n",
    "### Sum of All Parameters\n",
    "\n",
    "$\n",
    "210 + 110 + 110 + 110 + 110 + 11 = 661\n",
    "$\n",
    "\n",
    "Thus, in a compact form, the total number of trainable parameters is $ \\boxed{661} $.\n",
    "\n",
    "$\\text{\\# weights} = (20 * 10) + ... = 610 $\n",
    "\n",
    "$\\text{\\# bins} = 51$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad868e-2dde-4d9c-bac4-c55000880950",
   "metadata": {},
   "source": [
    "#### **&#x1F516;** **(&#x37;)** \n",
    "\n",
    "We would like to design an end-to-end communication system using an autoencoder. For that, we try to find a useful representation $r \\in \\mathbb{R}^r$ of the input $s \\in \\mathbb{R}^k$ at some intermediate layer through learning to reproduce the input at the output if $k = 5$, how much $n$ should be?\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b30b46-9de9-4686-804d-c6dffae920d6",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Given $ k = 5 $ (input dimension), a reasonable choice for the intermediate layer dimension \\( n \\) for an autoencoder would typically be less than the input dimension to achieve compression. Therefore, a compact representation for \\( n \\) would be:\n",
    "\n",
    "$\n",
    "n < k \\implies n < 5\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d79d0-6a2c-462e-9e91-01ad8bc04248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
