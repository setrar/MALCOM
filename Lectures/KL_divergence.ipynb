{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3407f06-8753-4649-aa21-41e6530eb216",
   "metadata": {},
   "source": [
    "The notation $ D(P \\parallel Q) $ usually refers to the **Kullback-Leibler divergence** (KL divergence), which is a measure of how one probability distribution $ P $ diverges from a second, expected probability distribution $ Q $. Essentially, it quantifies the amount of information lost when $ Q $ is used to approximate $ P $.\n",
    "\n",
    "### Definition\n",
    "The KL divergence from $ Q $ to $ P $ for discrete probability distributions is defined as:\n",
    "\n",
    "$\n",
    "D(P \\parallel Q) = \\sum_{x} P(x) \\log \\left(\\frac{P(x)}{Q(x)}\\right)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ P $ and $ Q $ are probability distributions.\n",
    "- The sum is over all possible events $ x $ in the distributions $ P $ and $ Q $.\n",
    "- $ \\log $ is typically the natural logarithm, but can be base 2 or 10 depending on the context or field.\n",
    "\n",
    "For continuous distributions, the sum is replaced by an integral:\n",
    "\n",
    "$\n",
    "D(P \\parallel Q) = \\int P(x) \\log \\left(\\frac{P(x)}{Q(x)}\\right) dx\n",
    "$\n",
    "\n",
    "### Properties\n",
    "1. **Non-Negativity**: $ D(P \\parallel Q) \\geq 0 $. The KL divergence is always non-negative, and $ D(P \\parallel Q) = 0 $ if and only if $ P = Q $ almost everywhere.\n",
    "2. **Asymmetry**: Note that $ D(P \\parallel Q) \\neq D(Q \\parallel P) $. This asymmetry means it is not a true metric.\n",
    "3. **Information Theoretic Interpretation**: KL divergence can be seen as the extra entropy introduced by assuming that the distribution is $ Q $ when the true distribution is $ P $, hence a measure of information loss.\n",
    "\n",
    "### Applications\n",
    "- **Machine Learning**: In machine learning, KL divergence is often used for algorithms like variational autoencoders (VAEs) where it helps in regularizing the models by minimizing the divergence between the learned model distribution and the actual data distribution.\n",
    "- **Statistical Inference**: It is used in Bayesian statistics to measure the divergence between the prior and the posterior, giving insights into how much information the data provides over the priors.\n",
    "- **Information Theory**: It measures the inefficiency of assuming that the distribution is $ Q $ when it is actually $ P $.\n",
    "\n",
    "### Example Calculation\n",
    "Suppose you have two discrete probability distributions:\n",
    "- $ P = [0.1, 0.4, 0.5] $\n",
    "- $ Q = [0.2, 0.3, 0.5] $\n",
    "\n",
    "The KL divergence $ D(P \\parallel Q) $ would be calculated as:\n",
    "$\n",
    "D(P \\parallel Q) = 0.1 \\log \\left(\\frac{0.1}{0.2}\\right) + 0.4 \\log \\left(\\frac{0.4}{0.3}\\right) + 0.5 \\log \\left(\\frac{0.5}{0.5}\\right)\n",
    "$\n",
    "\n",
    "This measure quantifies how much information is lost when using $ Q $ to represent $ P $. It is particularly useful in scenarios where the accuracy of an approximation to a probability distribution is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fa20e53-a5d4-4240-9f20-d791c9f6a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [0.1, 0.4, 0.5] \n",
    "Q = [0.2, 0.3, 0.5];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae0a7434-f6c6-468f-b315-7e0a0d322ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum (generic function with 10 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Printf\n",
    "∑ = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04d47ce7-89d2-4fbb-a7f9-9ae56208718e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kl_divergence (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate the KL divergence D(P || Q) using for-comprehension and zip\n",
    "function kl_divergence(P, Q)\n",
    "    # Ensure that both P and Q are valid probability distributions and are compatible\n",
    "    if length(P) != length(Q)\n",
    "        error(\"Distributions P and Q must have the same length\")\n",
    "    end\n",
    "    \n",
    "    # Using for-comprehension with zip to calculate KL divergence\n",
    "    return ∑(\n",
    "            p > 0 \n",
    "            && q > 0 ? p * log(p / q) : p > 0 \n",
    "            && q == 0 ? Inf : 0 \n",
    "            for (p, q) in zip(P, Q)\n",
    "        )\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd31ec5-2fc2-4065-aee1-8bcfda530e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The KL divergence D(P || Q) is 0.0458 bits"
     ]
    }
   ],
   "source": [
    "# Calculate KL divergence\n",
    "kl_result = kl_divergence(P, Q)\n",
    "\n",
    "# Print the result formatted as a floating point number\n",
    "@printf \"The KL divergence D(P || Q) is %.4f bits\" kl_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d8bb0-8dd6-4b63-8227-d423abef13fe",
   "metadata": {},
   "source": [
    "To illustrate the calculation of Kullback-Leibler divergence for continuous distributions in Julia, we'll use probability density functions (PDFs) and numerical integration since the KL divergence for continuous distributions involves integrals. Julia's `QuadGK` package, which provides methods for numerical integration, is well-suited for this purpose.\n",
    "\n",
    "### Example Setup\n",
    "\n",
    "Suppose we have two normal distributions:\n",
    "- Distribution $ P $ has a mean of 0 and a standard deviation of 1 (standard normal distribution).\n",
    "- Distribution $ Q $ has a mean of 1 and a standard deviation of 2.\n",
    "\n",
    "### KL Divergence for Continuous Distributions\n",
    "\n",
    "The KL divergence between two normal distributions $ \\mathcal{N}(\\mu_1, \\sigma_1^2) $ and $ \\mathcal{N}(\\mu_2, \\sigma_2^2) $ can be analytically calculated using the formula:\n",
    "\n",
    "$\n",
    "D(P \\parallel Q) = \\log\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n",
    "$\n",
    "\n",
    "For our specific example:\n",
    "- $ P = \\mathcal{N}(0, 1) $\n",
    "- $ Q = \\mathcal{N}(1, 2) $\n",
    "\n",
    "We can directly calculate it using Julia as follows:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ef8a11-33ce-4e98-90de-b45cfe361921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kl_divergence_normal (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate KL divergence for normal distributions\n",
    "function kl_divergence_normal(μ1, σ1, μ2, σ2)\n",
    "    term1 = log(σ2 / σ1)\n",
    "    term2 = (σ1^2 + (μ1 - μ2)^2) / (2 * σ2^2)\n",
    "    return term1 + term2 - 1/2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd82fbce-2a47-447e-bc76-20bed6579604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The KL divergence D(P || Q) is 0.4431 bits"
     ]
    }
   ],
   "source": [
    "# Mean and standard deviations for P and Q\n",
    "μP, σP = 0, 1\n",
    "μQ, σQ = 1, 2\n",
    "\n",
    "# Calculate KL divergence\n",
    "kl_result = kl_divergence_normal(μP, σP, μQ, σQ)\n",
    "\n",
    "# Print the result\n",
    "@printf \"The KL divergence D(P || Q) is %.4f bits\" kl_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9afc88-a46f-4cb6-a364-50b5f61f3d95",
   "metadata": {},
   "source": [
    "### Explanation of the Code\n",
    "\n",
    "1. **Function Definition**: The function `kl_divergence_normal` calculates the KL divergence between two normal distributions using the analytical formula specific to normal distributions.\n",
    "2. **Parameters**: Mean and standard deviation values for the distributions $ P $ and $ Q $ are defined.\n",
    "3. **Calculation and Output**: The KL divergence is calculated and printed.\n",
    "\n",
    "This approach uses the known analytical expression for KL divergence between normal distributions, which is much more efficient than attempting to numerically integrate the general formula for continuous distributions. For more complex distributions where no closed-form expression exists, numerical integration techniques would be necessary, involving defining the PDFs explicitly and using numerical integration methods to compute the integral. If you're interested in seeing how such a numerical integration could be approached in Julia, let me know, and I can provide an example using arbitrary PDFs and numerical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf63c8-cb24-44a2-9ea8-bbd1c3fe7cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
